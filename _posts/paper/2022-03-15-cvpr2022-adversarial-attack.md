---
layout: post
title: CVPR 2022有关对抗样本的论文集
category: 文献阅读
keyworks: adversarial examples, adversarial attack
tags: 对抗样本攻击
---

# Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon
## 背景

+ 常见的”粘贴式“的物理世界攻击存在以下两个问题：
  + 难以获取目标网络（hard to access the target model）
  + 无法打印数字空间所有的颜色（打印颜色不自然）
+ 目前的非侵入式攻击方法存在噪声不自然的问题

## 目的

设计了一种在黑盒设置下，生成由常见的自然现象（即阴影）构成的对抗扰动的方法。通过精心设计的阴影，可以成功攻击交通标志分类器。

## 方法

阴影设计的两个关键：

+ 阴影的位置

用点击V表示阴影区域，在使用掩膜M将其映射到图片中，构成对抗样本。作者指出由于在物理世界难以是实现复杂的形状， 同时作者表示简单的几何形状就足以达到很高的成功率。因此，作者采用了三角形的阴影。

+ 阴影的值

除了使用常见的EOT来实现物理攻击外。作者分析发现阴影和非阴影部分像素值之间的关系很难建模。为了简化问题，受到文献[17]的启发，作者假设阴影只影响光照通道，其他通道不受影响，因此作者首先要将RGB图片转换成LAB空间，然后动态的调整L通道实现对抗攻击。

![image-20220415164258212](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220415164258212.png)

攻击完之后，将LAB转换回RGB空间。

然而在现实世界中，阴影的是复杂的物理过程的结果。k的值可以由很多因素构成：

+ 光照源
+ 场景几何形状
+ 目标物体的材质
+ 相机的成像质量。

作者进一步分析了SBU阴影数据集中k的分布，发现k值在0.7以前的比例能达到94.25。在作者的方法中，在生成对抗样本之前，作者首先在目标物体上随机生成阴影来测试k的值。然后，进一步利用EOT，来减少k值的影响。

## 数字空间中的阴影攻击

作者在黑盒场景下对阴影的生成进行建模。作者假设攻击者仅能获得输入图像和输出（f的预测得分）。最终优化目标设置为

![image-20220415164947005](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220415164947005.png)

作者分析传统的0阶优化ZOO无法很好地解决问题，原因有如下两个：

+ 顶点的坐标是离散的
+ 判断坐标位置是否在掩膜内是一个二值问题，使渐变不可靠（不好求梯度？）。

为此，作者利用粒子群优化 （PSO） 策略，过程如下：

1、 维持一个粒子种群，每个粒子代表一个顶点的候选，以及在解决空间的移动速率

2、 在每次迭代过程中，所有粒子都根据当前单个最优和整个群体共享的全局最优值调整其速度和位置，最后更新每个最优值。

3、代价函数设置为模型f预测成真实值的得分。

4、停止策略：预测发生变化或达到最大迭代次数。

## 物理空间中的阴影攻击

### 1、EOT

![image-20220415165607756](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220415165607756.png)

### 2、预测稳定性

Then, we stabilize this prediction by rerunning PSO while conducting the following optimization

![image-20220415165626512](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220415165626512.png)

# Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity



## Motivation

生成对抗样本最直接的方式是利用梯度来生成对抗样本从而增加分类损失的代价。除此之外，还会使用Lp范数来约束干净图像和对抗样本图像之间的视觉差异。但是这些传统的方法存在以下两个问题：

+ Inherent limitation in cross-dataset generalization（跨数据集泛化的固有局限性）
+ HVS（人类视觉系统）糟糕的感知性。

现有方法与作者提出的方法之间的比较

![image-20220418143238772](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418143238772.png)

直觉上，规避分类图层的一种自然方法是在特征空间中执行攻击，因此作者提出了一种语义相似性攻击（SSA）。基本的假设是，高层的表示隐含着图像区分性和语义。SSA **focuses on perturbing semantic regions** such as objects in the scene while **suppressing redundant perturbations on irrelevant regions**.

the low-frequency component of an image contains the basic information, whereas the high-frequency components represent trivial details and noise.（图像的低频包括基础的信息，高频包括更精细的细节和噪声）。

为此，作者将低频部分作为不变的部分，并设计了一个约束项将扰动约束在高频组件上。尽管如此，文献[23]认为对抗样本可能即不在高频也不在低频，在低频扰动会产生更大的视觉差异，但是攻击效果更好。最近的文献也指出，在高频上构造的对抗样本不可感知性更好[39,43]。

不可感知性：Lp范数、频率域（高频）、SSIM，LPIPS等

## 方法

### 语义相似性攻击

不需要分类器的网络层，但是改变一对样本之间特征表示的相似性。

![image-20220418145103124](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418145103124.png)

![image-20220418145153513](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418145153513.png)

其中t表示最小批中目标图像的索引。公式4鼓励对抗样本在特征空间中靠近目标图片。**仅考虑非目标攻击**。（目标攻击没法做？）

为避免冗余扰动，作者设计了self-paced权重来提升优化的灵活性。

![image-20220418150109030](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418150109030.png)

 公式过于复杂，这么做带来的增益有多大？

### 低频约束

保持目标的基础信息并且将扰动限制在不可感知的高频

![image-20220418150235113](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418150235113.png)

做低频约束的原因：We observe that HVS is more sensitive to object structures and smooth regions, whereas it is not easy to perceive object edges and complex textures.

作为一种时频分析工具，离散小波变换（FWT）可以将图像解耦到一个低频和三个高频组件：

![image-20220418150442924](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418150442924.png)

其中L和H表示低通和高通录波器

作者在逆DWT时，丢弃了高频组件，并仅使用低频组件进行IDWT（**如果不做攻击，是否丢掉高频部也能实现攻击？**）。最后作者对从低频恢复的干净图片和对抗样本图片做了1范数的约束。

![image-20220418150726962](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418150726962.png)

Φ表示仅使用低频组件重建图像。

最终算法

![image-20220418150849445](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418150849445.png)

## 实验

数据集：CIFAR10/100,IMAGENET-1K

实现细节：扰动大小:8/255

评估指标：ASR，l2距离、l∞距离，FID、低频平均失真

### 白盒攻击

![image-20220418152250903](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418152250903.png)

### 鲁棒性

![image-20220418152257843](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418152257843.png)

抵抗对抗防御的效果较差

### 迁移性

![image-20220418152513786](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418152513786.png)

跟GD-UAP比没有意义，生成一个扰动和为每个图像生成一个扰动是没法比的。

![image-20220418152530298](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418152530298.png)

### 消融实验

损失的对比

![image-20220418152755479](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220418152755479.png)

## 总结

+ 本文使用了两种技术，
  + 特征上的攻击，最小化两对样本之间的相似度，即对抗样本与干净样本，以及对抗样本与其他样本之间的相似度。
  + 离散小波变换然后约束扰动在仅重建低频组件图像之间的差异，从而实现对**低频的约束**。
+ 方法中设计了目标攻击，但是并没有做相关的实验
+ 该文目的不明确，攻击效果不是最好，迁移性提升也不大，而且没有跟做迁移性的文章进行对比，说服力不强。

# Zero-Query Transfer Attacks on Context-Aware Object Detectors

## 1 Motivation

防御多目标的自然图像的对抗样本攻击的一种方式是通过**上下文检查**， 如果检测目标与出现的上下文不一致，则结果是可疑的。好的攻击算法需要欺骗这种上下文感知的检测器。作者提出了首个可以生成上下文感知黑盒对抗攻击方法，可以躲避黑盒检测器对复杂、自然场景下的上下文一致性检查。

但是不同于传统的基于重复查询检测器的攻击方法，作者假设了“零查询”的黑盒攻击方法。具体如下：

+ 首先，推导出多个攻击计划，将不准确的标签以上下文一致的方式指定给受害物体。
+ 然后，设计并使用了一种新颖的数据结构，作者称为**扰动成功概率矩阵（PSPM），这可以帮助筛除攻击计划，并选择一个最可能攻击成功的计划**。
+ 最后使用基于扰动的攻击算法来实现攻击（FGSM等）。

![image-20220421100425109](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421100425109.png)

本文，作者将注意力集中在目标共现上，这是建模语义上下文最基本的方法。上下文模型由共现图表示（共现矩阵），由给定的一组图像计算得到。**作者将一组目标作为上下文一致性，当且仅当相关的标签可以在共现图中构成一个全连接子图**。

## 2 Background

### 2.1  目标检测的上下文

提升视觉是吧的上下文以及得到了很多研究。基于共现矩阵的上下文信息源自图像像素、目标标签以及语言模型，已经被用于构建一个上下文感知的目标检测器。

### 2.2 黑盒攻击

基于迁移的黑盒对抗攻击。

### 2.3 目标检测攻击的

攻击目标检测器比攻击分类器更难，这是因为攻击需要同时攻击分类类别和物体的位置[55,57]。目标检测器隐含地使用了上下文信息[42,44]，例如，目标像素与背景像素之间的关系，以提升推断速度。

### 2.4 防御方法

最近的一些工作开始考虑在自然多目标场景下的上下文感知目标检测器的防御方法。虽然这些工作使用了与我们的工作不同的上下文概念，但它们证实了攻击上下文感知检测器更加困难。

### 3 Method

基于共现的上下文模型，用于派生多个上下文感知攻击计划，这些计划将一个或多个目标对象干扰到其各自的受害者标签。然后，考虑到扰动预算，攻击者使用预先计算的 PSPM 优化列表，我们将在后面详细讨论。结果是一个攻击计划，其中包含一个列表（受害者标签，目标标签）对，这些对最有可能成功欺骗受害者对象检测器。然后攻击者使用规避攻击算法，根据精细的攻击方案生成扰动场景。攻击成功的标准是：当且仅当目标物体被成功的扰动成目标标签，并且受害系统的目标检测器在检测列表中没有上下文一致性。

#### 3.1 上下文模型

![image-20220421102958971](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421102958971.png)

**上下文一致性**：如果在上下文图中的两个节点之间没有边相连，那么这两个标签不会在图片中同时出现。使用基于共现上下文，作者定义了上下文一致性和上下文不一致性。

+ 上下文一致性：目标之间的标签所表示的节点可以构建成上下文子图G。数据集中所有自然图像均满足这个条件。
+ 上下文不一致性：节点各自表示一种物体，上下文图中节点之间没有边相连。

![image-20220421103456818](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421103456818.png)

**一般性的上下文一致性**：假设利用阈值对G进行处理得到矩阵Hn，Hn中的值是当G中的值大于n，其余的均为0。那么更为一般性的上下文一致性可以定义成，物体的集合被认为是上下文一致性取决于这些物体的标签所表示的节点能否构成Hn的一个全连接子图。

### 3.3 PSPM

扰动成功概率矩阵PSPM是一个N×N的矩阵，表示成MC,ε,α，定义成一组集成的分类模型C，扰动大小ε以及目标扰动算法α。为具有N标签的特定训练集构建的PSPM。MC,ε,α(i,j)表示利用扰动攻击算法α使用攻击力大小ε将标签i成功扰动成j的概率。

PSPM表达了攻击者对场景中单个目标的攻击能力。PSPM的使用流程如下，假定给定场景中物体的集合为A。假定使用攻击算法α将集合A被扰动成集合B。上下文不可感知的攻击将从标签集合N中任意选择标签B。上下文感知的攻击将会确保集合B中的标签是上下文感知的，这由共现矩阵G确定。接着，通常存在一个或多个可能的扰动指定击 A → B，这些指定与上下文一致。依赖于训练数据集（不同姿势、尺寸、光照）一些目标扰动，Ai→Bj，比其他指定更容易攻击成功，尽管在白盒设置下。因此，通过检查共发生矩阵所建议的每个扰动赋值A->B具有不同的成功可能性。PSPM有助于选择最有可能的任务。一些选择的策略：

+ 选择A→B，最大化每个MC,ε,α（i，j）
+ 选择A→B，最大化所有MC,ε,α（i，j）的均值
+ 选择A→B，最大化所有MC,ε,α（i，j）中最小值

不失一般性，作者选择了第一种策略。

![image-20220421111537744](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421111537744.png)

### 3.3 上下文一致性攻击生成

接下来，将介绍如何使用共现矩阵G和PSPM矩阵M来设计上下文一致性攻击。整体流程图如图1所示，算法框架如算法1所示。

![image-20220421113742471](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421113742471.png)

![image-20220421113754487](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421113754487.png)

假定给场景中的物体指定目标（如将马改变成自行车）。如前所述，仅扰动一个对象就可能导致扰动场景中的对象列表上下文不一致。因此，需要将场景中的其他物体也扰动成某个物体以保证上下文一致性。接下来介绍的将确保不仅攻击计划是上下文一致性，而且这个计划是白盒攻击中最可能成功的。

## 3.4 算法流程

1. 攻击者**确定在场景中检测到的对象列表**，并使用**基于共现的上下文模型**推导出多个上下文感知攻击计划，这些攻击计划将一个或多个目标对象干扰到各自的受害者标签。
   + 共现矩阵G：给定具有N个标签的训练数据集，构建矩阵G，其中Gi,j表示在图像中标签i和j同时出现的数量。对G进行**行规范化**，得到矩阵G‘。G’中G‘i,j表示在观测到标签i的情况下，观测到标签j的概率，是一个条件概率。由此可定义上下文一致性，和上下文不一致性。
     + 上下文一致性：矩阵中网络有值的表示具有一致性
     + 上下文不一致性：矩阵中网络无值的表示不具有一致性
   + 广义上的上下文一致性：
     + 使用阈值η对共现矩阵G进行处理，保留矩阵中大于阈值η的值，得到Hη。如果表示对象标签的所有节点都形成上下文图 Hη 的完全连接子图，则在阈值η下，对象列表被视为上下文一致。
2. 给定攻击强度，攻击者利用算的PSPM来调整上述的列表。结果是一个攻击计划，其中包含一个列表（受害者标签，目标标签）对，这些对最有可能成功欺骗受害者对象检测器。
   + 扰动成功概率矩阵（PSPM）：NXN的矩阵，表示成MC,ε,α，即一组集成的分类模型C，以及扰动强度ε以及攻击算法α。即PSPM是帮助算法选择更加成功的攻击策略。

1. 然后攻击者使用攻击算法，根据精细的攻击方案生成扰动场景。
2. 扰动图像被发送到**具有显式上下文一致性检测机制的黑盒分类/检测机器**。
3. 攻击成功的条件：
   1. 图像中的物体均被扰动成预设的目标，且目标检测器的检测结果具有上下文一致性。



### 3.5 实现细节

使用PGD实现攻击

![image-20220421115550809](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421115550809.png)

![image-20220421115605872](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220421115605872.png)

## 4 实验

![image-20220422103247914](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220422103247914.png)

### 5 局限性

+ 通用性不强。图像中必须包含2个以上的目标。