---
layout: post
title: 【对抗样本（十二）】 Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability A Non-Robust Feature Perspective
category: 文献阅读
tags: BN对DNN的影响,对抗攻击，迁移性 
keywords: BN， robust features, non-robust features
---

---

关键词: BN, DNN的鲁棒特征和有用特征，BN使DNN倾向于学习有用特征，所以在干净数据集上的准确率很高。作者的发现可以增强迁移攻击。early stop 有助于提升对抗样本的迁移性。

---

## 摘要

BN被广发地用于DNN中，由于其有助于收敛。BN被观察到可以增加模型的准确率但是对对抗鲁棒性有害。ML社区逐渐开始研究BN对DNN的影响，特别是与鲁棒性相关的。本文舱室从非鲁棒特征的角度来理解BN对DNN的影响。直接地，提升的准确率可以归因于利用了更有效的特征。现在仍不清楚BN主要支持鲁棒特征(RFs)或非鲁棒特征（NRFs）。作者经济地发现BN将模型偏置至更加依赖NRFs。为分析这种特征鲁棒位移，作者提出一个框架将鲁棒有用解耦成鲁棒和有用。在提出地框架下，大量地分析证明DNN首先学习RFs，然后学习NRFs。另外一个发现是RFs的迁移性优于NRFs，这对构建迁移性更好的对抗样本攻击有一定意义。

## 引言

BN统计提升了模型对常见破坏的鲁棒性。相反，作者则通过聚焦从非鲁棒性特征的角度研究BNBN可以提升干净样本的准确率[35]，但是却以低鲁棒准确率为代价。直观上DNN可以看做一些有用特征，包括鲁棒特征（RFs）和非鲁棒特征（NRFs）[17]，准确率的提升可以粗略看作是模型利用更多有用特征。但是**尚未清楚BN支持学习RFs还是NRFs**。从作者的实验中得出，在标准训练中，**BN和其他正则变体均增加了对抗攻击的脆弱性，表明BN使模型在分类中更注意NRFs而不是RFs**。作者主张通过对坏的鲁棒性和特征可转移性的分析得到进一步证实。

由[26,28]证明鲁棒性和局部线性性的正向相关性，作者提出了Local Input Gradient Similarity（LIGS）指标，来衡量DNN局部线性性表征F鲁棒性的。LIGS提供了更直接的证明了BN影响学习到的F的鲁棒性。作者还比较了其他可能影响模型学习鲁棒性特征的因素，但是发现都不如LIGS有效。作者认为**其关键发现有益于寻找到增强迁移攻击**。

文献[11]发现BN增加了对抗脆弱性，作者发现这种脆弱性在LN/IN/GN等正则化变体中也存在。文献[43]认为干净样本和对抗样本是两个不同领域的数据，并对对抗样本使用一个附属批正则化来提升图像识别准确率。最近的研究表明，在推理阶段的协变量移位适应可以增加模型对常见破坏的鲁棒性。

**没有规范化的模型比有规范化的模型对噪声破坏的鲁棒性更好。这也说明了有规范化的模型学到的更多是有益于分类的特征，但是其代价是鲁棒性降低**。

## 方法

LIGS使用干净样本和对抗样本损失函数的梯度的余弦相似度进行衡量。

![image-20211103103844606](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211103103844606.png)

除非另有声明，作者从高斯分布中采样v来衡量LIGS。

1. 对抗训练最终训练了一个高F鲁棒性和低F有用性的模型；
2. 对于标准训练而言，移除BN也增加了F鲁棒性。
3. F鲁棒性，F有用性和F鲁棒有用性都可以通过LIGS来衡量。他们之间的关系如下图所示

![image-20211103104655288](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211103104655288.png)

**BN在对抗训练中的角色**：BN在对抗训练中的影响很有限。文献[43]人文BN可能会影响对抗训练时模型学到更鲁棒的特征。但是作者认为只有当同时使用干净样本的对抗样本时，这个现象才成立，这可能是由于干净样本和对抗样本分处两个数据域。**对于仅使用对抗样本的标准的对抗训练，BN的影响很有限**。

![image-20211103105744062](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211103105744062.png)

**LIGS正则化**：鲁棒准确率和LIGS是有关联的。LIGS作为正则化提升了模型的鲁棒性，但是在干净样本准确率上不明显。

![image-20211103105734655](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211103105734655.png)

下图显示了带有BN的模型是先学习RFs特征，然后学习NRFs特征。但是在训练的后期，模型逐渐忘记了开始学到的RFs特征。而不带BN的模型在整个阶段都只学习RFs特征，而忽略了NRFs特征。

![image-20211103110410962](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211103110410962.png)

##### BN之外其他因素对DNN行为的影响

**网络结构因素和优化因素**：

+ 结构类别
  + 网络宽度、深度：这二者的增加会增加或减少LIGS，但是幅度很小。
  + ReLU变体：ReLU和Leaky ReLU之间的没有明显差异，SeLU导致稍微较高的LIGS和较低的准确率。
+ 优化类别
  + 权重衰减：early and later stages of training 对LIGS影响较大。早期更高的权重衰减导致更高的LIGS，但是到后期逐渐降低。
  + 初始化学习率：更高的初始化率，导致早期更高的LIGS，但是最终会导致更低的LIGS。权重衰减和初始化学习率与干净样本的早期/晚期呈现处相反的趋势。
  + 优化器：SGD与ADAGrad相似，ADAM导致稍微更高的LIGS。

大部分被研究的因素都不如LIGS重要。

##### BN为什么支持NRFs以及如何帮助优化

+ （a）[31]认为平滑优化范围
+ （b） [1] 减少 internal covariate shift (ICS)

从作者的实验结论可知，参照[31]将梯度预测可视化，并发现它们不会像BN那样导致强梯度稳定性。 BN/IN/LN/GN 之间的一个共同点是它们都减少了 ICS，我们发现的证据支持观点 (b)。

##### 作者的发现对提升对抗迁移性的影响

最近的工作[30,40,37]显示了**鲁棒模型更加适合迁移学习的下游任务**，表明了包括鲁棒特征的模型能更好的迁移到不同任务。**一个自然的推测**是，此类模型也可能更适合用作替代模型，以生成跨模型的可转移对抗样本。我们的初步发现证实了这一猜想，即**在对抗训练模型上生成的对抗样本可以更好地转移到正常模型**。

