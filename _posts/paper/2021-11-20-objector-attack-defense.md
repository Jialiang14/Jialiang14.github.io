---
layout: post
title: 【文献调研】目标检测对抗样本攻击的防御方法
category: 文献阅读
keywords: 目标检测防御
tags: defense of adversarial attack against detector
---

# 【ICCV2021】Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes

关键词：**目标物体之间的一致性**来防御对抗样本攻击，场景描述->学习内在依赖

## 摘要

最近的研究表明检查输入数据的内在一致性关系是检测对抗攻击的有效方式（通过检查复杂场景中目标相互关系）。然而，现有的方法大多只对特定的模型联结，泛化性较差。基于**自然场景图像的语义描述**的观察，**使用语言模型来学习目标共现关系**，作者开发了一种使用语言模型来开发一种新颖方法来执行语言内容一致性检查。作者方法的区别性在于部署目标检测的独立性，在多目标实际场景中检测对抗的成功率。在VOC和COCO显示出性能由于其他方法。

简单来说，就是使用语言模型来学习复杂场景中对象之间的关联，利用这种差异来检测对抗样本。

## 概述

**当下主要的防御策略是捕获输入数据内部依赖**，并且检查这些依赖的违背来检测对抗样本。

上下文被广泛地应用于目标识别问题和场景理解，但是使用上文被用于检测对抗攻击的工作还较少。在作者之前的文献[21]中，提出将上下文建模成全脸截图，每个节点都是RPN中的一个提取框，边编码了其他区域（整个场景的背景）如何在特征空间影响当前节点。然后，**训练了一个自编码器（每个对应于一个目标类别）来检查上下文特征分布的一致性**。

在本文，作者提出一种新颖的基于目标共现的模型不可知的对抗攻击检测器。作者利用自然语言处理模型来学习共现目标之间的依赖性，并作为对学习上下文模型违法该规则的来检测对抗攻击。图1描述了方法的高层语义。作者首先**将检测网络的输出编码成描述成目标共现关系的句子（**车和信号）。然后，使用训练好的语言模型来预测每个检测到的基于上下文的实例。最终，通过比较语言模型的对检测结果的预测来评估场景图像的上下文一致性。如果结果存在差异，那么输出图像就是对抗样本。

![image-20211122104610915](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122104610915.png)

方法的例子

![image-20211122104717085](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122104717085.png)

方法的流程

![image-20211122104738181](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122104738181.png)

## 实验

实现细节：使用RoBERT模型作为基准模型来实现CENE-BERT，包括6个隐藏层和12个自注意头。

**PASCAL VOC**：该数据集包括20个类，大多数图像包括1到5个目标物体，平均每张图片包括1.4类和2.3个目标。检测模型的输出20 x 3x3=180。训练数据voc0712。

**COCO**：包括80个类，平均每张图片3.5个类，7.7个实例。使用gt标签来训练SCENE-BERT模型。模型的输出80x3x3=720。训练数据COCO14。

### 实验结果

![image-20211122122912229](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122122912229.png)

![image-20211122122925906](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122122925906.png)

案例研究

![image-20211122122941908](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211122122941908.png)