---
layout: post
title: 【对抗样本（九）】【USENIX】Why Do Adversarial Attacks Transfer Explaining Transferability of Evasion and Poisoning Attacks
category: 文献阅读
keywords: evasion/poisoning attack, transferability explaining, 
tags: 逃避/投毒攻击，对抗攻击迁移性解释
---

---

关键词：梯度对齐--皮尔森相关系数（针对替代模型/目标模型生成的对抗扰动），Pearson and Kendall coefficients. 替代对齐是衡量对抗样本迁移性最佳的方式。

---

##  摘要

在以前的工作中已经经验性证明了对抗样本具有迁移性，但是为什么攻击迁移性的潜在原因尚未很好的理解。在本文，作者进行了系统的分析，目的在于调查测试时逃避攻击和训练投毒攻击的迁移性。作者提出了一种通用优化框架来实现逃避和投毒攻击，并且给出了这些攻击的正式定义。作者指出了攻击迁移性的两个主要因素：

1. **目标模型内在的对抗脆弱性**
2. 用于优化攻击的**替代模型的复杂性**

基于这些观点，作者定义了三种影响攻击迁移性的指标。有趣的是，从理论分析得出的结果表明逃避和投毒攻击，可以通过一系列线性和非线性分类器和数据集进行实验确定。

本文的贡献：

+ 提出了一种逃避和投毒攻击的优化框架。框架支持不同对抗目标的威胁模型。基于该框架开发了一种针对逻辑回归的新颖的的基于梯度投毒攻击。投毒攻击比逃避攻击更难。
+ 迁移性定义和理论边界。给出了逃避和投毒攻击迁移性的正式定义，以及攻击成功的上界。这使我们能够推导出与模型复杂性相关的三个指标。揭露迁移性的正式定义依赖于
  + 目标分类器输入梯度的大小
  + 替代模型和目标模型梯度对齐情况
  + 生成攻击点的梯度范围的方差
+ 迁移性的完整实验验证：一系列分类器，包括逻辑回归，线性和RBF核的SVM，岭回归，随机森林，深度神经网络（前向和卷积神经网络）并在不同的应用数据集上：手写体识别、恶意软件检测、面部识别。
+ 迁移性的见解：攻击迁移性强烈依赖目标模型的复杂性，也就是内在脆弱性。这确认了减少输入梯度的尺寸，正则化，允许学习针对逃避的鲁棒性。其次，迁移性也受到替代模型和目标模型对齐的影响。最后，更稳定且具有较低方差的替代损失函数倾向于促进基于梯度的优化攻击以找到更好的局部最优。

作者提出的基于梯度的框架如下

![image-20211102213744304](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211102213744304.png)

## 相关结论

#### 输入梯度的大小

1. 输入梯度的尺寸依赖于给定模型的复杂度，由其正则化超参数控制。
2. 不那么复杂，但是很强正则化的分类器趋向于具有小的输入梯度，它可以学到更加平滑的函数，对于攻击更加鲁棒，反之亦然。
3. 通过控制权重衰减，减少输入梯度的平均大小，提升对逃避攻击的鲁棒性。（在MNIST89上训练的）
4. 由于复杂性是模型特征，在不同学习算法之间不能直接比较输入梯度的尺寸。
5. 如果分类器有大的输入梯度（由于输入空间的高维和正则化空间的低维），攻击者只需微小的，不可感知的扰动。

#### 梯度对齐

1. 这实际上是替代损失和目标损失梯度的余弦角。这可以解释为什么余弦角指标可以很好的表示对抗样本的迁移性，印证了之前的工作[21]。
2. 不同于梯度大小，梯度对齐是pairwise 指标，允许在不同替代模型之间比较。如果替代模型SVM与目标模型的梯度对齐比其他模型好，SVM替代模型生成的对抗样本就具有更好的迁移性

#### 梯度范围的变化

使用如下的梯度方差之间的变化范围作为衡量指标

![image-20211102220832593](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211102220832593.png)



## 实验

尽管在不同数据集和不同模型上进行了测试，但是都只是二分类模型。

相关实验参考论文。

部分结论如下：

1. 随机森林的脆弱性。值得一提的发现是随机森林可以被大部分其他模型的小扰动攻击。这个脆弱性是源自决策树分离节点时的使用的阈值造成的。
2. L2范数攻击几乎改变了所有特征的值，以较高的概率修改了树的每个路径上的特征，导致误分类。
3. 作者观察到**梯度对齐提供了迁移性的准确率衡量**：更高的余弦相似度，相关性越高。梯度对齐验证时非常快，因为它不需要模拟任何攻击，仅需要衡量攻击迁移性的相关性。
4. 有研究[5,20,41]相信神经网络由于其记忆能力，对公开投毒攻击具有更好的可靠性。
5. 模型复杂度在模型鲁棒性中扮演着一个重要的角色。
6. 错误率较其他case大的情况下，意味着攻击替代模型比攻击白盒模型的效果更好。
7. 事实上优化**攻击一个平滑的替代模型**可以找到在**目标模型上更好的局部最优值**。
8. 对于投毒攻击而言，减少替代模型的损失范围的变化的效果不如找到替代模型和目标模型之间好的对齐。
9. 当使用低复杂度线性替代模型实施攻击时，随机森林和NN一样对投毒攻击非常鲁棒。原因可能是目标分类行能力较大，可以较好的适应潜在样本，而不需要影响其他训练样本的精度。

## 思考

1. 作者仅证明了在二分类任务上逃避和投毒攻击的可迁移性的解释。在其他更复杂的任务上，该结论是否有效还需进一步研究。