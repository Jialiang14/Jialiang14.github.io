---
layout: post
title: 对抗样本和对抗样本防御中的有趣发现及解释
category: 文献阅读
keywords: findings on adversarial example, adversarial defense
tags: 对抗样本、对抗样本防御、发现、解释
---

# 【ICCV2021】Low Curvature Activations Reduce Overfitting in Adversarial Training

一句话概述：低曲率激活函数可以降低对抗训练中过拟合的问题

---

## 摘要

对抗训练是防御对抗样本攻击的一种非常有效的方式。之前的工作表明过拟合在对抗训练是一种支配现象，导致训练和测试具有很大的泛化差距。在本文，我们展示了使用低曲率的激活函数可以有效地降低对抗训练时鲁棒泛化性差距。作者观察到对于可微分/平滑激活函数如SiLU和非可微分/非平滑激活函数如LeakyReLU非常有效。对于后者而言，激活函数的近似曲率较低。最终，作者展示了使用低曲率的激活函数，对抗训练模型的双重下降现象不会发生。

## 引言

图1展示了在CIFAR-10模型，对于ReLU激活函数的对抗样本的测试误差在第一次学习率下降后会下降，然后持续的上升。

![image-20211219201626863](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219201626863.png)

## 方法

各个激活函数、其导数、其二次倒数

![image-20211219201903948](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219201903948.png)

测试样本的激活函数的最大特征值

![](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219201959949.png)



### 对抗训练时，激活函数曲率的影响

作者调查了以下几种激活函数

![image-20211219202143541](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219202143541.png)

各个激活函数的学习曲线图

![image-20211219202232541](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219202232541.png)



## 结论

+ 使用低曲率激活函数的鲁棒性更好，并具有更小的泛化差距。
+ 输入海森矩阵的特征值越小，对抗鲁棒性越高
+ 激活函数的曲率对鲁棒性和标注泛化误差有直接的影响
+ 相比于ReLU，SiLU取得了几乎相同鲁棒性和更高的标注准确率
+ **训练的更长**和**增加网络结构尺寸**可以看作是**增加模型复杂度**，因此在这二者设置下可以观察到双下降现象。[48]



# 【ICCV2021】Relating Adversarially Robust Generalization to Flat Minima

一句话概述：作者探究了对抗鲁棒性泛化性与平坦最小值的关系

---

## 摘要

对抗训练存在严重的过拟合：在对抗样本上的交叉熵损失，又称鲁棒损失，在训练样本时持续下降，但是在测试阶段时确实不断上升。实际上，只会导致糟糕的鲁棒泛化性，如对抗鲁棒性无法很好地泛化到新样本上。在本文，作者**在权重空间**研究了**鲁棒泛化性与鲁棒损失景观平坦性之间的关系**，如当扰动权重时鲁棒损失发生巨大变化。为此，作者提出了平均平均和最糟糕例子标量来衡量鲁棒损失景观的平坦性，并且展示**好的鲁棒泛化性和平坦性之间的关系**。例如，考虑训练，当发生过拟合时，平坦度大大降低，这就是早停策略可以有效找到鲁棒损失景观中的平坦最小值。相似的，AT变体取得了更高的对抗鲁棒性与其对应的平坦最小值。这支持很多常见的选择，如AT-AWP，TRADES，MART，使用自监督的AT或者额外无标注样本的AT，抑或是样本正则化技术，如AutoAugment，权重酸碱或者标签噪声。为公平的比较这些方法，作者发现平坦性策略是位尺度不变特别设计的，并且进行了大量的实验来验证我们的发现。

![image-20211219225856974](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219225856974.png)

## 引言

鲁棒过拟合典型的现象是鲁棒损失（RLoss）或鲁棒测试错误（RErr），如在对抗样本上的交叉熵损失和测试错误率。结果是，鲁棒性泛化性差距，测试和训练鲁棒性之间的差异会越来越大。

![image-20211219230332657](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219230332657.png)

如图2所示，测试样本上的对抗鲁棒性最终开始下降，然而训练样本上的鲁棒性持续上升。

![image-20211219230901257](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219230901257.png)

正常情况下，在测试样本上的最优权重不与在训练样本上找到的最小值并不一致。因此，平坦性会造成好的泛化性，因为**测试样本上的损失并没有巨大的提升**（如图3右侧中小的泛化差距）。文献[34]发现视觉平坦与更好的泛化性有关。作者发现：**早停策略仍然是必要的**。

贡献：

在本文，作者研究了**在权重空间的鲁棒损失的平坦性提升了鲁棒性泛化性**。为此，作者提出了基于平均和最糟糕平坦测量方法来提升鲁棒例子，因此面对了尺度不变性的挑战，在顶层给出了RLoss并权重扰动，及RLoss和RErr之间的差异。作者展示了鲁棒性泛化提升与平坦性一致，反之则相反。图1绘制了RLoss（越低越鲁班，y轴）以及RLoss平均平坦性（越低越平坦，x轴）。

+ 平坦最小化，损失景观与权重的变化，被认为有助于提升标准的泛化性[23]。
+ [34]展示了ResNet中残差连接或权重衰减导致视觉的平坦最小化.
+ [41,28]将该概念构建成平均case和最糟糕case。

## 方法

参考原文

## 实验

### 鲁棒过拟合的对比

![image-20211219234313069](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20211219234313069.png)