---
1layout: post
title: 【对抗样本（二十二）】ADVERSARIAL SPHERES
category: 文献阅读
keywords: adversarial examples
tags: 对抗样本，对抗样本解释
---

# 摘要

尽管有大量的研究探究对抗样本，但是引起对抗样本的原因尚未明了。作者假设这种反直觉行为是数据流形的高维几何形状的自然结果。探索该假设的第一步，作者研究了两个同心高维球体合成数据分类问题。对于此数据集，作者展示了测试错误大小和与邻近错误的平均距离之间的trade-off。实际上，作**者证明用于对球体的一小部分分类都容易受到$O(1/\sqrt d)$对抗扰动的影响**。此外，作者在该数据集上训练了不同结构的网络，他们的错误集几乎都自然地落在该理论边界内。作为理论的结果，机器学习模型对微小对抗扰动的脆弱性是观测到的测试误差量的逻辑结果。作者希望他们在简单情况下的理论分析将有助于探索**复杂的真实世界数据集的几何如何产生对抗样本**。

# 1 Introduction

大量工作证明标准图像模型展现出如下的现象：从数据分布中随机选择的大部分图像都能被正确的分类，然而，它们接近视觉上相似的附近图像却容易被分错[Goodfellow,2014]。这种现象通常被称为对抗样本。这些对抗性发现的错误可以被构建成非常鲁棒的，对视点，方向和尺度不变[Athalye，2017]。尽管有一些理论工作和对抗防御策略的工作，但是对该现象的理解仍然很糟糕。

关于对抗样本的原因，已经有一些假设。作者对此进行了简单的回顾。一种常见的假设是神经网络分类器在输入空间的各个区域过于线性[Goodfellow]。其他一种假设是对抗样本远离了数据流形[Goodfellow,Anonymous,Lee]。Cisse认为内在权重矩阵大的奇异值可能导致分类器对输入中的微小扰动很脆弱。

除了努力解释对抗样本的工作外，一些工作通过更改使用的非线性[Krotov,2017]来增加对微小扰动的鲁棒性，或使用正则化[cisse，2017]。其他工作使用第二个统计模型来检测对抗样本。然而，这些方法中很多都无法防御CW攻击。最终，对抗训练显示出在很多场景中都能提升鲁棒性。尽管在提升对对抗扰动鲁棒性上取得了很多进步，但是局部误差仍然显示出现在刚好超出对抗训练范围的距离[sharma，2017]。

该现象非常有趣，因为这些模型在测试数据集上取得了很高的准确率。作者假设这个行为是在高维数据流形中自然存在的结果。为了研究该假设，作者定义了一种简单的合成数据分类任务，即在两个高维球体之间进行分类。这有助于在数据流形具有很好的数学解释的设置中研究对抗样本，并且有助于分析由模型学到的决策边界的特征。更重要的是，作者可以很自然地改变数据流形地维数，并且研究输入维度对神经网络地泛化误差几何的影响。作者在合成数据上的实验和理论分析如下：

+ 图像模型呈现出相似的行为：**从数据分布中随机选择的大部分点均能正确地被分类，而那些彼此接近的通常容易被分错**。这种现象在测试错误率小于1/100w的情况下仍会出现。
+ 对于该合成数据集，在泛化误差的大小和到最近错误的平均距离之间存在重要的权衡。特别的，作者展示了任何对球体的一分部区域误分类的模型都容易受到大小为$O(1/\sqrt d)$对抗扰动的影响。
+ 在该数据集上训练的神经网络很自然地接近权衡误差集地衡量与最近误差平均距离的理论最优值。这意味着为线性地增加到最近错误地平均距离，模型的错误率必须指数级的减少。

最后，作者将详细讨论对抗样本对球体模型与图像模型之间的联系。

# 2 The concentric spheres dataset

数据分布由数学地描述成两个d维同心圆：生成随机样本x，x的二范数为1或者R，为每个范数分配相等的概率（本文R=1.3）。作者关联每个x一个目标y，如如果||x||2=1，那么又，||x||2=R，y=0。（一个球体的半径为1，另一个球体的半径为R=1.3？）

研究一个合成的高维数据集有很多优势：

+ 数据的概率密度p（x）有很好的定义，在支撑中的所有x上统一。作者也能通过在分布z ~N（0，1）上均匀的采样p(x)，并且设置x=z/||z||2或x=Rz/||z||2。
+ 存在理论上界，这能很好地分割两个类别（球体的半径为(R+1)/2）.
+ 作者设计的机器学习模型被证明可以学到分割两个球体的的决策边界。

R=1.3的选择是随意的，作者并未仔细的探究对抗样本与两个球体距离之间的关系。作者选择数据分布限制为两个领域的目的是为了进一步简化问题。

作者在实验研究了两种模式在该数据集的训练效果。

+ 第一种是online setting，每个最小批均是从p(x)中均匀采样的结果。
+ 第二种固定了大小为N的训练集，模型在该有限集上训练了多个epoch。

图1是数据分布

![image-20220217143153653](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217143153653.png)

# 3 Adversarial examples for a deep relu network

作者的第一个实验使用的输入维度d=500。然后训练了一个两个隐藏层的ReLU网络，其中每个隐藏层有1000个神经元，并且对每个隐藏层使用了BN，但是不是readout layer。作者训练了minibatch SGD, 最小化sigmoid 交叉熵损失。作者使用Adam优化器，100百万训练步骤，最小批大小为50，学习率0.0001。由于使用了在线设置，训练时一共使用了5000千万数据点。

作者在从球体采样的1000千万个均匀样本上验证了最终模型，一共使用了2000千万数据，并且在这些有限样本上没有观测到误差。因此，该模型的错误率是未知的，仅有错误率的统计上界。尽管如此，在该球体上利用梯度反传可以对抗性地发现数据流形上的误差。本文使用了两种类型的对抗样本，

+ 第一种是最糟糕样本，迭代攻击直到攻击目标收敛并且没有限制到开始点的局部区域。
+ 第二种是最接近样本，找到第一个误分类攻击的时候就停止。

在图1 ，作者可视化了决策边界，通过对500维空间采取不同的2d投影。当使用随机投影时，模型非常接近投影的最大边界。注意模型很自然地在两个球体之间进行插值，尽管只在球体表面地样本上接受训练。与此相反，当对有一个偏置向量有一个最糟糕对抗样本进行2D投影时，模型的决策边界非常高度贴近该“对抗方向”。模型确信> 2 的范数点位于内球上。作者还取了一个切片，其中 x 轴和 y 轴是跨越到两个不同最坏情况示例的子空间的正交基础。虽然最后一个图显示外球的整个投影被错误分类，但由于高维空间，这个误差区域的体积非常小。

尽管非常罕见，但这些错误分类似乎接近球体上随机采样的点。到数据流形上最接近的误差的平均 L2 距离为 0.18，相比之下，内球上的两个随机采样点通常彼此之间的距离约为 $\sqrt 2 \approx 1.41$。如果在两个球体之间寻找被归类为在外球体上的最近点，那么得到的平均L2距离为0.0796，平均范数为1.07。因此，**另一类最接近的样本通常是到理论边距的距离的一半左右**。

这种不太可能但局部误差的现象只有在球体是高维时才会出现。 在图 2（右图）中，作者可视化了在 d = 2 的情况下在 100 个样本上训练的相同模型。该模型在数据流形上不出错。在实验中，作者能够训练ReLU网络的最高维度，其中没有可以对抗性地发现错误（局部或非本地）似乎在d = 60左右。

![image-20220217145749025](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217145749025.png)

## 3.1 Find adversarial examples with a manifold attack

最近的一些工作假设对抗样本是流行之外的样本。作者将测试这种流形之外的样本。为此，作者设计了一种攻击，特地在流形上构造对抗样本，称为流形攻击。传统的对图像模型的攻击方法始于输入x和目标类别y‘，找到在约束||x-x'||<ε下，对抗样本x’使P(y'|x')最大，范数通常维无穷范数。作者使用PGD来解决上述约束问题，仅在投影不揍，作者通过正规化||x'||2将对抗样本投影回球体。由于该攻击仅为数据流形生成对抗样本，它们在数据分布下的概率等于正确分类的点。p(x) = p(x')

# 4 Analytic forms for a simpler network

很难推断出ReLU网络的学习决策边界。为了更全面地了解决策边界，作者研究一个更简单的模型。该网络被称为"二次网络"，是一个单隐藏层网络，其中逐点非线性是二次函数，σ（x）= x方。隐藏层中没有偏差，输出只是对隐藏激活求和，乘以标量并添加偏差。对于隐藏维度 h，网络具有 d ×h + 2 个可学习参数。logit可以写成

![image-20220217151745991](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217151745991.png)

其中$W_1 \in R^{hxd}$, 1是h的列向量。最终，w和b都是学习的标量。在附录种，作者表示网络的输出可以写成以下的形式

![image-20220217151927040](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217151927040.png)

其中αi是标量，却决于模型的参数，向量z是输入x的旋转。二次网络的决策边界为令公式2为0得到的等式。它是与原始同心的d维椭球体。这允许作者分析模型是否有对抗样本。特别的，如果αi＞1，那么错误就在球体内部。如果任意αi＜1/R^2^，那么错误就在球体外部。因此，当且仅当αi∈[1/R^2^,1]时，模型有完美的准确率。

当使用第3部分相同的设置训练当h=1000时的二次网络时，获得了完美的解决方案，即α∈[1/R^2^,1]并且不存在对抗样本。这同样是在在线学习设置中，其中每个小基站都是数据分布中的IID样本。然而，如果在数据分布的有限样本上进行训练，情况就不同了。特别是如果样本点来自p(x)，大小为N=10^6^ 作为固定有限训练集以及使用相同的设置进行训练，作者得出一个经验上具有非常低错误率的模型 - 从每个球体随机抽样1000万个数据点不会产生任何错误，但有对抗性示例。实际上，在500中的394上学习到的αi并不在[1/R^2^,1]中（完整的直方图看图3）。

作者使用中心极限定理（CLT）从i（第4.1节）估计二次网络的错误率。此特定模型的估计错误率约为10^-11^。请注意，作者在分布的尾部应用 CLT，因此尚不清楚此估计值的准确性。然而，作者发现CLT非常接近该制度中的错误率，其中它足够大，可以进行数值估计。

接下来，作者通过"完美"初始化增强了上述设置;在一个点上初始化二次网络，对于该点，所有i都是"正确的"，但由于sigmoid交叉熵损失而存在非零梯度。网络初始化在内球和外球的 y = 1 的 sigmoid 概率分别为 .0016 和 0.9994 的点上。如图 3 所示，从此初始化开始的持续训练会导致最坏和平均情况损失的快速发散。虽然测试集上的平均损失随着进一步训练而减少，但最坏的情况迅速增加，并且在1000个训练步骤后可以再次找到对抗性示例。这种行为是由于训练目标（平均sigmoid交叉熵损失）不直接跟踪模型的准确性这一事实造成的。它还展示了当输入为高维时，最坏情况和平均情况损失如何可能分化。

## 4.1 Analystic estimates of the error rate

作者使用CLT理论分析二次网络对αi的估计的准确性。以下命题估计内球体上的错误率

![image-20220217154431064](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217154431064.png)

命题 4.1 意味着 i 的许多设置可以获得非常低的错误率。只要 E[αi] 约等于（1+ R^-2^）=2）且它们的方差不会太高，模型会非常准确。 图3中的直方图说明了这一点;即，学习模型的错误率为10􀀀11，但80%的i不正确。对于典型样本，模型将不正确的数字相加并得到正确答案。选择i的灵活性，同时保持良好的精度，随着输入尺寸的提高而显著增加。

为了说明这一点，进一步考虑二次网络的特殊情况，其中决策边界的形式为

![image-20220217154703629](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217154703629.png)

此简化模型有两个参数，k 是模型查看的维度数，b 是分隔两个类的阈值。k 需要多大才能使模型获得所需的错误率？（假设 b 是基于 k 以最佳方式选择的）。作者使用命题 4.1 中的 CLT 近似来回答这个问题。在图4中，作者绘制了使用此简化模型获得所需精度所需的输入尺寸分数。例如，如果 d = 3000，则模型可以获得 10^-14^ 的估计精度，而只查看输入的 50%。当作者训练一个二次网络时，这种行为就会在经验上发生，该二次网络太小而无法完全分离两个球体。例如，具有 1000 个隐藏节点且用 d = 2000 训练的二次网络在内部和外部球面上分别获得 10^-21^ 和 10^-9^ 的估计泛化误差，尽管其大小仅为实现 0 误差所需的 1/2。这表明，实现非常低的测试误差所需的网络大小可能明显小于实现0测试误差所需的网络大小。

# A small amount of classification error implies local adversarial examples

如第3节所示，在球体数据集上训练的神经网络表现出与图像数据集相似的现象：数据分布中的大多数随机样本都经过正确分类，并且接近附近的错误分类点。在这项工作中，作者不试图将自然图像流形的几何形状与球体的几何形状进行比较，但作者可以解释为什么这个属性出现在球体数据集上。

令S0 位于 d 维半径为 1 的球体中，并固定 $E \subseteq S_0$（将 E 解释为内球体上被某个模型错误分类的点集）。对于 x ∈S0，设 d（x;E） 表示x和集合E中最近点的L2距离。

![image-20220217155715429](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217155715429.png)

![image-20220217155746332](https://gitee.com/freeneuro/PigBed/raw/master/img/image-20220217155746332.png)

# 6 Conclusion and takeways

在这项工作中，作者试图通过研究更简单的合成数据集来深入了解图像模型的对抗性示例的存在。在此数据集上训练不同的神经网络架构后，作者观察到与图像模型类似的现象 - 数据分布中的大多数随机点都已正确分类并且接近错误分类点。然后，作者通过证明模型的错误率与独立于模型的到最接近的误差的平均距离之间的理论权衡来解释此特定数据集的这种现象。作者还观察到，几种不同的神经网络架构与这一理论边界非常接近。

定理5.1很重要，因为它将为什么模型容易受到对抗性示例的问题减少到为什么存在少量分类误差的问题。目前尚不清楚像定理5.1这样的东西是否适用于图像流形，未来的工作应该调查类似的原则是否适用。如果类似的原则确实适用，那么这可以解释为什么尽管有大量的研究兴趣，但解决对抗性示例问题却如此困难。例如，最近的一项工作使用对抗训练来增加L1指标的鲁棒性（Madry等人，2017）。尽管这确实增加了可靠地产生误差所需的扰动的大小，但局部误差仍然大于那些对抗训练的误差（Sharma&Chen，2017）。最近提出了几种针对对抗性例子的防御措施，这些辩护的动机是假设对抗性的例子脱离了数据流形（匿名，2018b;a;Lee et al.， 2017）。本文
结果质疑这一假设是否普遍成立。如第3节所示，有 数据流形打开和关闭的本地错误，数据流形上的本地错误处于打开状态 平均系数为 2，远于数据流形误差。

这项工作提出了许多问题，即**在有限的数据下是否有可能完全解决对抗性示例问题**。重要的是要注意，**一个足够大的二次网络，用足够的数据训练，最终会变得完美**。但是，第 4.1 节中的一个教训是，**实现 0 分类误差所需的网络大小可能明显大于实现非常小的分类误差所需的网络大小。也许在非常大的图像数据集上训练的足够大的神经网络最终会变得完美**。

当然，同心球数据集是一个非常简单的问题，它不太可能捕获自然图像流形几何的所有复杂性。然而，我们希望从这个非常简单的案例中获得的见解将指明前进的道路，以探索复杂的现实世界数据集的几何形状如何导致对抗性的例子。