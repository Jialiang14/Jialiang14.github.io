---
layout: post
title: 机器学习/深度学习中常见的衡量指标
category: 深度学习技术
tags: evolution mertics
keywords: AUC, ACC, ROC, PR
---

---

### 前言 本文主要介绍了机器学习/深度学习中常用模型指标

​	 		---- (参考博客)



### 1. 混淆矩阵

在机器学习领域和统计分类问题中，混淆矩阵是可视化工具，特别用于监督学习，在无监督学习一般叫作匹配矩阵。矩阵的每一列代表一个类实例预测，而每一行表示一个实际的类的实例。如下表所示是一个三分类系统的预测结果（主对角线的值表示系统预测正确，对角线以外的值表示系统预测错误）

|                |        | **实际的类别** |        |        |
| -------------- | ------ | -------------- | ------ | ------ |
|                |        | **猫**         | **狗** | 猪     |
| **预测的类别** | **猫** | **15**         | 12     | 8      |
|                | **狗** | 11             | **13** | 10     |
|                | 猪     | 9              | 12     | **16** |

在预测分析中，混淆矩阵通常是具有两行两列的表，如下。

|                |                | **实际的类别**                            |                                       |
| :------------: | -------------- | ----------------------------------------- | ------------------------------------- |
|                |                | Positive（正样本/阳性）                   | Negative（负样本/阴性）               |
| **预测的类别** | Positive（真） | **True** <br />实际为正类<br />预测为正类 | False<br />实际为负类<br />预测为正类 |
|                | Negative（假） | False<br />实际为正类<br />预测为负类     | True<br />实际为负类<br />预测为负类  |

很多评价指标都源自于上表的简化版（下表）

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | TP             | FP       |
|                | Negative | FN             | TN       |

+ T**P**： 实际类别为正类，系统预测也为**正**类（分类正确）
+ T**N**：实际类别为负类，系统预测也为**负**类（分类正确）
+ F**P**：实际类别为负类，系统预测也为**正**类（分类错误）
+ F**N**： 实际类别为正类，系统预测也为**负**类（分类错误）

一般而言，系统的TP和TN越高，FP和FN越低越好，这表示系统可以准确地区分正负样本。

### 2. 准确率（Accuracy）

准确率是表征混淆矩阵中正对角线上被正确分类的样本之和占总样本的比率。

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | **TP**         | FP       |
|                | Negative | FN             | **TN**   |

​			                	$Accuracy = \frac{TP+TN}{TP+FP+TN+FN}$， 也即$Accuracy = \frac{TP+TN}{样本总数}$

### 3. 精确率（查准率，Precision）

精确率定义为，从模型预测的角度上看，`在所有预测为正样本中，标签与真实样本同为正样本的比例`，表示模型对于预测正确的**置信度**。（在下表中，TP占该行的比例，横着看）

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | **TP**         | **FP**   |
|                | Negative | FN             | TN       |

​									        		$Prcision = \frac{TP}{TP+FP}$

### 4. 召回率（查全率，Recall）

召回率定义为，`从真实标签的角度上看，在所有的正样本中，被模型正确地预测为正样本所占的比例`

（在下表中，TP占该列的比例，竖着看）

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | **TP**         | FP       |
|                | Negative | **FN**         | TN       |

​															$Recall= \frac{TP}{TP+FN}$

### 5. F1得分（调和精确率与召回率）

观察`Precision`与`Recall`的公式，可以看出这两个公式的分子都相同（`同为TP`），其计算都是围绕着正样本的预测性能来计算的，同时`Precision`与`Recall`之间存在不可调和的矛盾（由于分类阈值的不同，`FP`和`FN`的值不同），如下表所示：

| 样本编号 | 真实类别 | 模型预测值 | 阈值为0 | 阈值为0.9 |
| -------- | -------- | ---------- | ------- | --------- |
| 1        | 1        | 0.9        | 1       | 1         |
| 2        | 1        | 0.8        | 1       | 0         |
| 3        | 1        | 0.7        | 1       | 0         |
| 4        | 1        | 0.6        | 1       | 0         |
| 5        | 1        | 0.5        | 1       | 0         |
| 6        | 1        | 0.4        | 1       | 0         |
| 7        | 0        | 0.3        | 1       | 0         |
| 8        | 0        | 0.2        | 1       | 0         |
| 9        | 0        | 0.1        | 1       | 0         |
| 10       | 0        | 0.05       | 1       | 0         |

**当阈值为0时，预测结果如下表所示**

| 阈值为0        |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | 6              | 4        |
|                | Negative | 0              | 0        |

Precision = 6 / (6 + 10) = 0.6

Recall = 6 / (6 + 0) = 1

**当阈值为0.9时，预测结果如下表所示**

| 阈值为0.9      |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | 1              | 0        |
|                | Negative | 5              | 4        |

Precision = 1 / (1+ 0) = 1

Recall = 1 / (1 + 5) = 0.33

分析上述结果，可以得到以下结论：

+ 当系统将所有样本全部预测为正样本时（系统过于贪婪，预测值大等于0即为正样本），显然召回率可以达到最大，取值1，但是精确率将显著下降
+ 当设定很大的阈值（系统过分保守，预测值大等于0.9才是正样本），仅有一个正样本被预测正确，那么精确率的取值为1，但召回率将很低（`负样本被分为正类的概率降低，很多正样本被分类成负类`）

为了调和`Precision`与`Recall`的之间的矛盾，将引入F1指标来综合反映模型的性能：

​                            $\frac{2}{F1} = \frac{1}{Precision} + \frac{1}{Recall}$, $F1=\frac{2 * Prcision * Recall}{Prcision + Recall}$

此外，还有一个更通用的计算$F_\alpha$值得公式

 $F_\alpha=\frac{1+\alpha^2}{\alpha^2*Prcision + Recall}*Prcision * Recall$

当$\alpha=1$时，即为F1。

### 6. 真阳性率 - TPR（真正率）

真阳性率TPR（True Positive Rate）定义为预测正确的正样本占实际正样本的比例（`给定数据集标签后，统计出正样本个数即可作为分母`）：

​															$TPR= \frac{TP}{TP+FN}$

可以发现，TPR=Recall

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | **TP**         | FP       |
|                | Negative | **FN**         | TN       |

### 7. 假阳性率 - FPR（假正率）

假阳性率 - FPR（False Positive Rate）预测错误的正样本占实际负类的比例（`给定数据集标签后，统计出负样本个数即可作为分母`）

​															$FPR= \frac{FP}{TP+FN}$

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | TP             | **FP**   |
|                | Negative | FN             | **TN**   |

对比TPR和FPR的计算公式可知，**其分母要么是全部的正样本，要么是全部的负样本**，增加正样本的个数，不会影响FPR的计算，反之依然，二者互不干扰，因此 常说**TPR和FPR对于样本不平衡不敏感**。

### 8. KS

KS(Kolmogorov-Smirnov)值，计算公式如下，KS能够反映出模型最优的区分效果，以及对应的阈值：

​																	$KS = max(TPR - FPR)$

### 9. 灵敏度 - Sensitivity

计算结果与TPR, Recall相同。在医学诊断中，“阳性”可以表示“染病”，“阴性”可以表示“健康”。

> 包括医学诊断检验的许多测试中，灵敏度是指真阳性没有被忽视的程度（所以伪阴性很少），而特异度是真阴性确实鉴别的程度（所以伪阳性很少）。因此，一个高灵敏度的检验很少忽略真阳性（例如：即使有异常仍然检验为无异常）；而高特异度检验则很少将不是检验目标的其他东西鉴别为阳性（例如：检验出一种非常相似的细菌却将其误判为目标细菌）。一个高灵敏度*且*高特异度的检验表示其两方面都做得好，所以这个检验“很少忽略它正在寻找的目标*并且*很少将其他东西误判为目标。”

### 10. 特异度 - Specificity

特异度的计算如下图所示：

​											$特异度 = \frac{TN}{FP + TN}$

|                |          | **实际的类别** |          |
| -------------- | -------- | -------------- | -------- |
|                |          | Positive       | Negative |
| **预测的类别** | Positive | TP             | **FP**   |
|                | Negative | FN             | **TN**   |

**特异度 = TN / (TN + FP) = 1 - FPR**

### 从点到线 - ROC曲线

由于混淆矩阵是在确定了一个阈值后生成的，因此，利用不同的阈值就可以得到多组（FPR, TPR）点，将这些点依次连接起来，就构成了ROC曲线（`没有必要先遍历阈值，如从0~1之间，将阈值切分1000等分，可以直接根据样本预测值从大到小排序，依次遍历样本即可，得到数据中的阈值`），受试者工作特征曲线ROC曲线的横坐标是FPR，纵坐标是TPR。

下面通过一个论文中的一个例子来说明ROC是怎么画出来的，数据如下，一共20个样本，正样本10个，负样本10个，每个样本具有不同的模型预测值（得分），根据模型预测值从大到小进行排序；

| Inst # | Class | Score | Inst# | Class | Score |
| ------ | ----- | ----- | ----- | ----- | ----- |
| 1      | p     | .9    | 11    | p     | .4    |
| 2      | p     | .8    | 12    | n     | .39   |
| 3      | n     | .7    | 13    | p     | .38   |
| 4      | p     | .6    | 14    | n     | .37   |
| 5      | p     | .55   | 15    | n     | .36   |
| 6      | p     | .54   | 16    | n     | .35   |
| 7      | n     | .53   | 17    | p     | .34   |
| 8      | n     | .52   | 18    | n     | .33   |
| 9      | p     | .51   | 19    | p     | .30   |
| 10     | n     | .505  | 20    | n     | .1    |

ROC曲线流程：假定L为带标签和模型预测值的数据集，其中正样本个数为P，负样本个数为N，样本$i$的模型分记为$f(i)$，用$R$存放（FPR, TPR）点：

1. 根据模型预测值对数据集L进行从大到小排序，得到$L_{sort}$，第一个样本记为$i=1$，由于TPR和FPR的分母已知，故只需要初始化分子TP，FP为0，模型预测值的阈值初始化为$f_{pre}=\infin$；

2. 依次对$L_{sort}$的样本进行遍历：

   2.1）如果满足$f(i)\neq f_{pre}$（相邻样本的模型预测值取值不相同），则$R.append(\frac{FP}{N},\frac{TP}{P})$，更新

   $f(i)=f_{pre}$；

   2.2）如果$L_{sort}[i]$的真实标签和预测标签都为正样本，那么$TP = TP + 1$，否则$FP = FP + 1$更新样本下标$i=i+1$;

3. 将最后一个点加入到$R.append((1,1))$

有了上述的计算流程，对于上述数据，前几个点的FPR，TPR取值如下：

1. 当阈值为$\infin$，显然TP=0，FP=0，(0，0)，计算过程：
   + 对于FPR和TPR处处相等，那么ROC会是一条直线（理解为，模型会将一半的正样本，负样本预测错误，相当于随机猜测了）；
   + 对于点（1，1），阈值为$-\infin$，由于样本模型预测值子啊[0,1]之间，那么所有样本被预测为正样本（混淆矩阵第二行全为0），TP=P（正样本个数），FP=N（负样本个数）。
   + 对于点（0，0），阈值为$\infin$，所有样本都被预测为负样本（混淆矩阵第一行全为0），TP=0,FP=0;
   + 阈值越小时，召回率\TPR就越高，FPR也越高。
2. 遍历第一个样本，阈值设置为0.9，将样本预测值标签设为正，和真实标签都为正样本，因此$TPR=\frac{1}{10} = 0.1, FPR = 0, (0,0.1)$；
3. 遍历到第二个样本，阈值设置为0.8，$TPR=\frac{2}{10} = 0.2, FPR = 0, (0,0.2)$；
4. 遍历到第三个样本，阈值设置为0.7，将样本预测值标签设为正，和真实标签不同，$FP=1，TPR=\frac{2}{10} = 0.2, FPR = \frac{1}{10}, (0.1,0.2)$；

最终整个ROC曲线如下图所示：

由于样本个数有限，可以发现ROC曲线其实是一个阶跃形状，了解到这一点，有助于计算ROC曲线下的面积（AUC）。

### 从点到线 - PRC曲线

Precision - Recall 曲线，简称PRC，其横坐标为Recall，纵坐标为Precision，曲线的绘制方式与ROC类似。

+ PRC的横坐标是ROC的纵坐标（Recall VS TPR）

### ROC - AUC (ROC曲线下的面积)

##### 计算方式1

AUC是ROC曲线下的面积，直接根据整个定义，对ROC计算流程进行简单改进，就可以得到AUC的计算流程：

+ 根据模型预测值对数据集L进行排序，得到$L_{sort}$，第一个样本记为$i=1$，由于TPR和FPR的分母已知，故只需要初始化分子TP，FP为0，由于是不断积分的过程（计算提醒面积的过程），还需要记录上一次$TP_{prev}$,$FP_{prev}$的值，模型预测值的阈值初始化为$f_{pre}=\infin$，面积为A；
+ 依次对$L_{sort}$的样本进行遍历：
  + 如果满足$f_{pre} \neq \infin$ （相邻样本模型预测值得取值不相同），则$A=A+TRAPEZOID(FP,FP_{prev},TP,TP_{prev})$，更新$f_{pre}=f(i);FP_{prev}=FP,TP_{prev}=TP$;
  + 如果$L_{sort}[i]$的真实标签和预测标签都为正样本，那么$TP=TP +1$，否则$FP=FP +1$更新样本下标$i=i+1$
+ $A=A+TRAPEZOID(N,FP_{prev},N,TP_{prev})$
+ 缩放回去：$A = \frac{A}{P \times N}$

其中上述$TRAPEZOID(X_1,X_2,Y_1,Y_2)$是计算梯形面积的过程：

$base = |X_1 - X_2|$

$height = \frac{Y_1+Y_2}{2}$

$area = base \times height$

##### 计算方式2

AUC实际上是模型将正样本排在负样本前面的概率，随机从正负样本中分别选出一个样本，构成样本对，样本对中正样本的模型预测值大于负样本模型预测值的占比，即为AUC

### sklearn计算的接口

| 接口                   | 说明                                            |
| ---------------------- | ----------------------------------------------- |
| confusion_matrix       | 计算混淆矩阵来评估分类的准确性                  |
| accuracy_score         | 模型准确度（Accuracy）                          |
| f1_score               | 计算F1得分                                      |
| precision_recall_curve | 针对不同的概率阈值计算精度(precision)，召回率对 |
| precision_score        | 计算精确度                                      |
| recall_score           | 计算召回率                                      |
| roc_auc_score          | 计算特征曲线（ROC AUC）下预测分数的计算区域     |
| roc_curve              | 计算ROC                                         |

### 总结

+ 判断一个指标是否受正负样本量的影响，看这个指标在混淆矩阵中是否只利用单独某列上的数据；如果是， 对正负样本采样，不影响模型的指标
+ 在正负样本极不平衡的情况下，PR曲线ROC曲线更能反映模型的性能。